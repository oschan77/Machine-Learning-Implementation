{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7WvIt4t97xMI"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import make_blobs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hS7Dh_vw4qCW"},"outputs":[],"source":["# X: (m, n)\n","# y: (m, 1)\n","# w: (n, 1)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":378,"status":"ok","timestamp":1691469551057,"user":{"displayName":"Wanghin Chan","userId":"14848871187079720320"},"user_tz":-480},"id":"-dcuWBQd4xCR","outputId":"167808c1-550a-4021-a0d1-d66afa426eef"},"outputs":[{"name":"stdout","output_type":"stream","text":["<EP1> Loss: 0.6931471805599452\n","<EP50> Loss: 0.0025852674226038977\n","<EP100> Loss: 0.001384040865973791\n","<EP150> Loss: 0.000950693116395078\n","<EP200> Loss: 0.0007258611855710866\n","<EP250> Loss: 0.0005878670122963997\n","<EP300> Loss: 0.0004944071060572587\n","<EP350> Loss: 0.000426855440254162\n","<EP400> Loss: 0.00037571739477784195\n","<EP450> Loss: 0.00033563984370887245\n","<EP500> Loss: 0.0003033729519367404\n","w.shape: (4, 1)\n","w: [[ 0.17660363]\n"," [ 0.03040468]\n"," [-0.79738696]\n"," [ 0.50720101]]\n","b: 0.033354095240112375\n","Accuracy: 100%\n"]}],"source":["class LogisticRegression:\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","        self.lr = 0.1\n","        self.epochs = 500\n","        self.m, self.n = X.shape[0], X.shape[1]\n","\n","    def cost(self, y_pred):\n","        return - 1 / self.m * np.sum(self.y * np.log(y_pred) + (1 - self.y) * np.log(1 - y_pred))\n","\n","    def compute_grads(self, y_pred):\n","        grad_w = 1 / self.m * np.dot(self.X.T, (y_pred - self.y))\n","        grad_b = 1 / self.m * np.sum(y_pred - self.y)\n","\n","        return grad_w, grad_b\n","\n","    def sigmoid(self, x):\n","        return 1 / (1 + np.exp(-1 * x))\n","\n","    def accuracy(self, y_argmax):\n","        return np.sum(y_argmax == self.y / self.m)\n","\n","    def predict(self):\n","        y_pred =  self.sigmoid(np.dot(self.X, self.w) + self.b)\n","        y_argmax = y_pred > 0.5\n","\n","        acc = self.accuracy(y_argmax)\n","\n","        return y_pred, acc\n","\n","    def train(self):\n","        self.w = np.zeros((self.n, 1))\n","        self.b = 0\n","\n","        for i in range(self.epochs):\n","            ep = i + 1\n","            y_pred, _ = self.predict()\n","            loss = self.cost(y_pred)\n","            if ep%50 == 0 or ep == 1 or ep == self.epochs:\n","                print(f'<EP{ep}> Loss: {loss}')\n","            grad_w, grad_b = self.compute_grads(y_pred)\n","            self.w -= self.lr * grad_w\n","            self.b -= self.lr * grad_b\n","\n","        print(f'w.shape: {self.w.shape}')\n","        print(f'w: {self.w}')\n","        print(f'b: {self.b}')\n","\n","if __name__ == \"__main__\":\n","    X, y = make_blobs(n_samples=200, n_features=4, centers=2)\n","    y = np.expand_dims(y, axis=1)\n","    logReg = LogisticRegression(X, y)\n","    logReg.train()\n","\n","    _, acc = logReg.predict()\n","\n","    print(f'Accuracy: {acc}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1PQqhw2cQl2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Deo4xKak4ptw"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNxFltI93dn3B4nd7Fqpb/B","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
