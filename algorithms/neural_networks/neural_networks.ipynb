{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPpMC3Z10tVJAjy3nsTvSvp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"q2YFUbFkCXQn"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import make_blobs"]},{"cell_type":"code","source":["class NeuralNetwork:\n","    def __init__(self):\n","        self.lr = 0.1\n","        self.epochs = 500\n","\n","    def weights_init(self, input_size, output_size):\n","        W = np.random.randn(input_size, output_size)\n","        b = np.random.randn(1, output_size)\n","\n","        return W, b\n","\n","    def ReLU(self, z):\n","        return np.maximum(0, z)\n","\n","    def Softmax(self, z):\n","        exp_scores = np.exp(z)\n","        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n","\n","        return probs\n","\n","    def cost(self, probs, m, y):\n","        return np.sum(-np.log(probs[np.arange(m), y]) / m)\n","\n","    def forward_props(self, X, y, params):\n","        W1 = params[\"W1\"]\n","        b1 = params[\"b1\"]\n","        W2 = params[\"W2\"]\n","        b2 = params[\"b2\"]\n","\n","        z1 = np.dot(X, W1) + b1\n","        a1 = self.ReLU(z1)\n","\n","        z2 = np.dot(a1, W2) + b2\n","        probs = self.Softmax(z2)\n","\n","        cache = {\n","            \"a0\": X,\n","            \"a1\": a1,\n","            \"probs\": probs,\n","        }\n","\n","        return probs, cache\n","\n","    def backprops(self, cache, params, y, m):\n","        W1 = params[\"W1\"]\n","        b1 = params[\"b1\"]\n","        W2 = params[\"W2\"]\n","        b2 = params[\"b2\"]\n","\n","        a0 = cache[\"a0\"]\n","        a1 = cache[\"a1\"]\n","        probs = cache[\"probs\"]\n","\n","        dz2 = probs\n","        dz2[np.arange(m), y] -= 1\n","        dz2 /= m\n","\n","        dW2 = np.dot(a1.T, dz2)\n","        db2 = np.sum(dz2, axis=0, keepdims=True)\n","\n","        dz1 = np.dot(dz2, W2.T)\n","        dz1 = dz1 * (a1 > 0)\n","\n","        dW1 = np.dot(a0.T, dz1)\n","        db1 = np.sum(dz1, axis=0, keepdims=True)\n","\n","        grads = {\n","            \"dW1\": dW1,\n","            \"dW2\": dW2,\n","            \"db1\": db1,\n","            \"db2\": db2,\n","            }\n","\n","        return grads\n","\n","    def params_update(self, params, grads):\n","        W1 = params[\"W1\"]\n","        b1 = params[\"b1\"]\n","        W2 = params[\"W2\"]\n","        b2 = params[\"b2\"]\n","\n","        dW1 = grads[\"dW1\"]\n","        db1 = grads[\"db1\"]\n","        dW2 = grads[\"dW2\"]\n","        db2 = grads[\"db2\"]\n","\n","        W1 -= self.lr * dW1\n","        b1 -= self.lr * db1\n","        W2 -= self.lr * dW2\n","        b2 -= self.lr * db2\n","\n","        params = {\n","            \"W1\": W1,\n","            \"b1\": b1,\n","            \"W2\": W2,\n","            \"b2\": b2,\n","        }\n","\n","        return params\n","\n","    def train(self, X, y):\n","        m, n = X.shape\n","        h1 = 16\n","        h2 = len(np.unique(y))\n","        W1, b1 = self.weights_init(n, h1)\n","        W2, b2 = self.weights_init(h1, h2)\n","\n","        params = {\n","            \"W1\": W1,\n","            \"b1\": b1,\n","            \"W2\": W2,\n","            \"b2\": b2,\n","        }\n","\n","        for ep in range(self.epochs):\n","            probs, cache = self.forward_props(X, y, params)\n","\n","            loss = self.cost(probs, m, y)\n","            if ep+1 == 1 or ep+1 == self.epochs or (ep+1)%50 == 0:\n","                print(f'<EP{ep+1}> Loss: {loss}')\n","\n","            grads = self.backprops(cache, params, y, m)\n","            params = self.params_update(params, grads)\n","\n","if __name__ == \"__main__\":\n","    X, y = make_blobs(n_samples=500, n_features=5, centers=5)\n","    nn = NeuralNetwork()\n","    nn.train(X, y)"],"metadata":{"id":"o2lyi3OXXpBx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691560996797,"user_tz":-480,"elapsed":308,"user":{"displayName":"Wanghin Chan","userId":"14848871187079720320"}},"outputId":"08553339-9355-4d42-c1e1-6284f43f0c2a"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["<EP1> Loss: 48.249937433930924\n","<EP50> Loss: 0.0228202032025797\n","<EP100> Loss: 0.012298731359709249\n","<EP150> Loss: 0.00793503133837336\n","<EP200> Loss: 0.005658178248127125\n","<EP250> Loss: 0.004347574067854021\n","<EP300> Loss: 0.0035159710305199686\n","<EP350> Loss: 0.002938905273345385\n","<EP400> Loss: 0.002512796724371166\n","<EP450> Loss: 0.002185183304272873\n","<EP500> Loss: 0.0019256285816536991\n"]}]}]}